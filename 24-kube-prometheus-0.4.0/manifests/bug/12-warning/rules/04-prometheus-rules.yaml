apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-rules
  namespace: monitoring
spec:
  groups:
  - name: Prometheus.rules
    rules:
    - alert: PrometheusAllTargetsMissing
      expr: count by (job) (up) == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        title: 'Prometheus all targets missing'
        description: "A Prometheus job does not have living target anymore."

    - alert: PrometheusConfigurationReloadFailure
      expr: prometheus_config_last_reload_successful != 1
      for: 0m
      labels:
        severity: warning
      annotations:
        title: 'Prometheus configuration reload failure'
        description: "Prometheus: 【{{ $labels.instance }}】 configuration reload error."

    - alert: PrometheusTooManyRestarts
      expr: changes(process_start_time_seconds{job=~"prometheus-k8s|pushgateway|alertmanager-main"}[15m]) > 2
      for: 0m
      labels:
        severity: warning
      annotations:
        title: 'Prometheus too many restarts'
        description: "Prometheus: 【{{ $labels.instance }}】 has restarted more than twice in the last 15 minutes. It might be crashlooping."

    - alert: PrometheusAlertmanagerConfigurationReloadFailure
      expr: alertmanager_config_last_reload_successful != 1
      for: 0m
      labels:
        severity: warning
      annotations:
        title: 'Prometheus AlertManager configuration reload failure'
        description: "AlertManager: 【{{ $labels.instance }}】 configuration reload error"

    - alert: PrometheusNotificationsBacklog
      expr: min_over_time(prometheus_notifications_queue_length[5m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        title: 'Prometheus notifications backlog'
        description: "Prometheus: 【{{ $labels.instance }}】 The notification queue has not been empty for 10 minutes"

    - alert: PrometheusAlertmanagerNotificationFailing
      expr: rate(alertmanager_notifications_failed_total[5m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        title: 'Prometheus AlertManager notification failing'
        description: "AlertManager: 【{{ $labels.instance }}】 is failing sending notifications"

    - alert: PrometheusTsdbCheckpointCreationFailures
      expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[5m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        title: 'Prometheus TSDB checkpoint creation failures'
        description: "Prometheus: 【{{ $labels.instance }}】 encountered {{ $value }} checkpoint creation failures"

    - alert: PrometheusTsdbCheckpointDeletionFailures
      expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[5m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        title: 'Prometheus TSDB checkpoint deletion failures'
        description: "Prometheus: 【{{ $labels.instance }}】 encountered {{ $value }} checkpoint deletion failures"

    #- alert: PrometheusTsdbCompactionsFailed
    #  expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
    #  for: 1m
    #  labels:
    #    severity: critical
    #  annotations:
    #    title: 'Prometheus TSDB compactions failed'
    #    description: "Prometheus: 【{{ $labels.instance }}】 encountered {{ $value }} TSDB compactions failures"

    - alert: PrometheusTsdbHeadTruncationsFailed
      expr: increase(prometheus_tsdb_head_truncations_failed_total[5m]) > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        title: 'Prometheus TSDB head truncations failed'
        description: "Prometheus: 【{{ $labels.instance }}】 encountered {{ $value }} TSDB head truncation failures"

    #- alert: PrometheusTsdbReloadFailures
    #  expr: increase(prometheus_tsdb_reloads_failures_total[1m]) > 0
    #  for: 1m
    #  labels:
    #    severity: critical
    #  annotations:
    #    title: 'Prometheus TSDB reload failures'
    #    description: "Prometheus: 【{{ $labels.instance }}】 encountered {{ $value }} TSDB reload failures"
